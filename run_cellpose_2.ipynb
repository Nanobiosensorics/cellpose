{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MouseLand/cellpose/blob/main/notebooks/run_cellpose_2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7c7V4yEqDc_"
   },
   "source": [
    "# Running cellpose 2.0 in colab with a GPU\n",
    "\n",
    "<font size = 4>Cellpose 2.0 now allows you to train your own models in the GUI!\n",
    "\n",
    "This notebook allows you to load this **custom model** and run the model on your images with a GPU. \n",
    "\n",
    "In this notebook, you can also **train** a custom model using your labels (`_seg.npy`) files, or other labels as `_masks.tif` files. If you already have a trained model, skip this part of the notebook.\n",
    "\n",
    "For more details on cellpose 2.0 check out the [paper](https://www.biorxiv.org/content/10.1101/2022.04.01.486764v1) or the [talk](https://www.youtube.com/watch?v=3ydtAhfq6H0).\n",
    "\n",
    "Mount your google drive to access all your image files, segmentations, and custom models. This also ensures that any models you train are saved to your google drive. If you'd like to try out the notebook without your own files, please download the sample images from tissuenet (optional step in Setup below).\n",
    "\n",
    "This notebook was inspired by the Zero-Cost Deep-Learning to Enhance Microscopy project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories. Please check out their great work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvyuR08OZfw4"
   },
   "source": [
    "# Setup\n",
    "\n",
    "We will first install cellpose 2.0, check the GPU is working, and mount google drive to get your models and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbqFni8kuFar"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUfSFlZgp1aV"
   },
   "source": [
    "Install cellpose -- by default the torch GPU version is installed in COLAB notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2cBEO1PLuO7"
   },
   "source": [
    "Check CUDA version and that GPU is working in cellpose and import other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tt8hgC7rniP8",
    "outputId": "677fa3d0-952f-4490-f5bb-4ef1ad0b0469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Thu_Nov_18_09:45:30_PST_2021\n",
      "Cuda compilation tools, release 11.5, V11.5.119\n",
      "Build cuda_11.5.r11.5/compiler.30672275_0\n",
      "Sat Jun  3 18:19:53 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   66C    P0    N/A /  N/A |    671MiB /  4096MiB |     49%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1664      G   /usr/lib/xorg/Xorg                376MiB |\n",
      "|    0   N/A  N/A      2050      G   /usr/bin/gnome-shell               68MiB |\n",
      "|    0   N/A  N/A      4250      G   ...373571546660890287,262144      138MiB |\n",
      "|    0   N/A  N/A     23004      G   ...RendererForSitePerProcess       82MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      ">>> GPU activated? YES\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi\n",
    "\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import core, utils, io, models, metrics\n",
    "from glob import glob\n",
    "\n",
    "use_GPU = core.use_gpu()\n",
    "yn = ['NO', 'YES']\n",
    "print(f'>>> GPU activated? {yn[use_GPU]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQI4aUxCjz3n",
    "outputId": "804d0459-b120-4298-9b4c-87a9ca26401c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default advanced parameters enabled\n"
     ]
    }
   ],
   "source": [
    "#@markdown ###Path to images and masks:\n",
    "\n",
    "train_dir = \"human_in_the_loop/train\" #@param {type:\"string\"}\n",
    "test_dir = \"human_in_the_loop/test\" #@param {type:\"string\"}\n",
    "#Define where the patch file will be saved\n",
    "base = \"/content\"\n",
    "\n",
    "# model name and path\n",
    "#@markdown ###Name of the pretrained model to start from and new model name:\n",
    "from cellpose import models\n",
    "initial_model = \"cyto\" #@param ['cyto','nuclei','tissuenet','livecell','cyto2','CP','CPx','TN1','TN2','TN3','LC1','LC2','LC3','LC4','scratch']\n",
    "model_name = \"CP_tissuenet\" #@param {type:\"string\"}\n",
    "\n",
    "# other parameters for training.\n",
    "#@markdown ###Training Parameters:\n",
    "#@markdown Number of epochs:\n",
    "n_epochs =  100#@param {type:\"number\"}\n",
    "\n",
    "Channel_to_use_for_training = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "# @markdown ###If you have a secondary channel that can be used for training, for instance nuclei, choose it here:\n",
    "\n",
    "Second_training_channel= \"None\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "\n",
    "#@markdown ###Advanced Parameters\n",
    "\n",
    "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
    "#@markdown ###If not, please input:\n",
    "learning_rate = 0.1 #@param {type:\"number\"}\n",
    "weight_decay = 0.0001 #@param {type:\"number\"}\n",
    "\n",
    "if (Use_Default_Advanced_Parameters): \n",
    "  print(\"Default advanced parameters enabled\")\n",
    "  learning_rate = 0.01 \n",
    "  weight_decay = 0.0001\n",
    "  \n",
    "#here we check that no model with the same name already exist, if so delete\n",
    "model_path = train_dir + 'models/'\n",
    "if os.path.exists(model_path+'/'+model_name):\n",
    "  print(\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n",
    "  \n",
    "if len(test_dir) == 0:\n",
    "  test_dir = None\n",
    "\n",
    "# Here we match the channel to number\n",
    "if Channel_to_use_for_training == \"Grayscale\":\n",
    "  chan = 0\n",
    "elif Channel_to_use_for_training == \"Blue\":\n",
    "  chan = 3\n",
    "elif Channel_to_use_for_training == \"Green\":\n",
    "  chan = 2\n",
    "elif Channel_to_use_for_training == \"Red\":\n",
    "  chan = 1\n",
    "\n",
    "\n",
    "if Second_training_channel == \"Blue\":\n",
    "  chan2 = 3\n",
    "elif Second_training_channel == \"Green\":\n",
    "  chan2 = 2\n",
    "elif Second_training_channel == \"Red\":\n",
    "  chan2 = 1\n",
    "elif Second_training_channel == \"None\":\n",
    "  chan2 = 0\n",
    "\n",
    "if initial_model=='scratch':\n",
    "  initial_model = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8SDv9XztBgb"
   },
   "source": [
    "Here's what the command to train would be on the command line -- make sure if you run this locally to correct the paths for your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQsv-Iz7m_CF",
    "outputId": "bef9d0e1-ca51-450c-8c60-856c3f545bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m cellpose --use_gpu --verbose --train --dir human_in_the_loop/train --pretrained_model cyto --chan 0 --chan2 0 --n_epochs 100 --learning_rate 0.01 --weight_decay 0.0001 --test_dir human_in_the_loop/test --mask_filter _seg.npy\n"
     ]
    }
   ],
   "source": [
    "run_str = f'python -m cellpose --use_gpu --verbose --train --dir {train_dir} --pretrained_model {initial_model} --chan {chan} --chan2 {chan2} --n_epochs {n_epochs} --learning_rate {learning_rate} --weight_decay {weight_decay}'\n",
    "if test_dir is not None:\n",
    "    run_str += f' --test_dir {test_dir}'\n",
    "run_str += ' --mask_filter _seg.npy' # if you want to use _seg.npy files for training\n",
    "print(run_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JRxBPmatrK7"
   },
   "source": [
    "## Train new model\n",
    "\n",
    "Using settings from form above, train model in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import CellDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files\n",
    "# train_dir = '../data'\n",
    "# output = io.load_train_test_data(train_dir, None, mask_filter='_seg.npy')\n",
    "# train_data, train_labels, _, test_data, test_labels, _ = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/balint/projects/cellpose/human_in_the_loop/train/img_5.tif', '/home/balint/projects/cellpose/human_in_the_loop/train/img_3.tif', '/home/balint/projects/cellpose/human_in_the_loop/train/img_1.tif', '/home/balint/projects/cellpose/human_in_the_loop/train/img_4.tif', '/home/balint/projects/cellpose/human_in_the_loop/train/img_0.tif']\n",
      "['/home/balint/projects/cellpose/human_in_the_loop/train/img_5_seg.npy', '/home/balint/projects/cellpose/human_in_the_loop/train/img_3_seg.npy', '/home/balint/projects/cellpose/human_in_the_loop/train/img_1_seg.npy', '/home/balint/projects/cellpose/human_in_the_loop/train/img_4_seg.npy', '/home/balint/projects/cellpose/human_in_the_loop/train/img_0_seg.npy']\n",
      "['/home/balint/projects/cellpose/human_in_the_loop/train/img_5_flows.tif', '/home/balint/projects/cellpose/human_in_the_loop/train/img_3_flows.tif', '/home/balint/projects/cellpose/human_in_the_loop/train/img_1_flows.tif', '/home/balint/projects/cellpose/human_in_the_loop/train/img_4_flows.tif', '/home/balint/projects/cellpose/human_in_the_loop/train/img_0_flows.tif']\n"
     ]
    }
   ],
   "source": [
    "train_loader = CellDataLoader('/home/balint/projects/cellpose/human_in_the_loop/train/', shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcYskYudMajM",
    "outputId": "ed4dce67-190e-4d52-9a70-bcbc76be4b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 18:20:03,271 [INFO] WRITING LOG OUTPUT TO /home/balint/.cellpose/run.log\n",
      "2023-06-03 18:20:03,273 [INFO] \n",
      "cellpose version: \t2.2.1 \n",
      "platform:       \tlinux \n",
      "python version: \t3.8.5 \n",
      "torch version:  \t2.0.1+cu117\n",
      "2023-06-03 18:20:03,274 [INFO] >> cyto << model set to be used\n",
      "2023-06-03 18:20:03,278 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2023-06-03 18:20:03,280 [INFO] >>>> using GPU\n",
      "2023-06-03 18:20:03,618 [INFO] >>>> model diam_mean =  30.000 (ROIs rescaled to this size during training)\n",
      "2023-06-03 18:20:03,659 [INFO] computing flows for labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 18:20:04,010 [INFO] computing flows for labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 18:20:04,404 [INFO] computing flows for labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 18:20:04,786 [INFO] computing flows for labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 18:20:05,098 [INFO] computing flows for labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 18:20:05,304 [INFO] >>>> median diameter set to = 30\n",
      "2023-06-03 18:20:05,305 [INFO] >>>> mean of training label mask diameters (saved to model) 19.259\n",
      "2023-06-03 18:20:05,342 [INFO] >>>> training network with 2 channel input <<<<\n",
      "2023-06-03 18:20:05,344 [INFO] >>>> LR: 0.01000, batch_size: 8, weight_decay: 0.00010\n",
      "2023-06-03 18:20:05,346 [INFO] >>>> ntrain = 5\n",
      "2023-06-03 18:20:05,348 [INFO] >>>> nimg_per_epoch = 5\n",
      "2023-06-03 18:20:05,354 [INFO] Epoch 0/100::   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "2023-06-03 18:20:11,025 [INFO] Epoch 0/100::  50%|#####     | 1/2 [00:05<00:05,  5.67s/it]\n",
      "2023-06-03 18:20:11,521 [INFO] Epoch 0/100:: 100%|##########| 2/2 [00:06<00:00,  2.63s/it]\n",
      "2023-06-03 18:20:11,565 [INFO] Epoch 0/100:: 100%|##########| 2/2 [00:06<00:00,  3.10s/it]\n",
      "2023-06-03 18:20:11,566 [INFO] Epoch 0, Time  6.2s, Loss 2.7990, LR 0.0000\n",
      "2023-06-03 18:20:11,567 [INFO] saving network parameters to human_in_the_loop/train/models/CP_tissuenet\n",
      "2023-06-03 18:20:11,764 [INFO] Epoch 1/100::   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "2023-06-03 18:20:13,635 [INFO] Epoch 1/100::  50%|#####     | 1/2 [00:01<00:01,  1.87s/it]\n",
      "2023-06-03 18:20:13,990 [INFO] Epoch 1/100:: 100%|##########| 2/2 [00:02<00:00,  1.02it/s]\n",
      "2023-06-03 18:20:14,183 [INFO] Epoch 1/100:: 100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "2023-06-03 18:20:14,185 [INFO] Epoch 1, Time  8.8s, Loss 2.5238, LR 0.0011\n",
      "2023-06-03 18:20:14,186 [INFO] saving network parameters to human_in_the_loop/train/models/CP_tissuenet\n",
      "2023-06-03 18:20:14,384 [INFO] Epoch 2/100::   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "2023-06-03 18:20:16,596 [INFO] Epoch 2/100::   0%|          | 0/2 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/balint/projects/cellpose/run_cellpose_2.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# set channels\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m channels \u001b[39m=\u001b[39m [chan, chan2]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m new_model_path \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(save_path\u001b[39m=\u001b[39;49mtrain_dir, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                               n_epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                               learning_rate\u001b[39m=\u001b[39;49mlearning_rate, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                               weight_decay\u001b[39m=\u001b[39;49mweight_decay, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                               nimg_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                               model_name\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                               )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# diameter of labels in training images\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/balint/projects/cellpose/run_cellpose_2.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m diam_labels \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdiam_labels\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/projects/cellpose/cellpose/models.py:953\u001b[0m, in \u001b[0;36mCellposeModel.train\u001b[0;34m(self, save_path, save_every, save_each, learning_rate, n_epochs, patience, momentum, SGD, weight_decay, batch_size, nimg_per_epoch, model_name)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" train network with images train_data \u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \n\u001b[1;32m    931\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39m# train_data, train_labels, test_data, test_labels, run_test = transforms.reshape_train_test(train_data, train_labels,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m#                                                                                            test_data, test_labels,\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m#                                                                                            channels, normalize)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39m# if channels is None:\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[39m#     models_logger.warning('channels is set to None, input must therefore have nchan channels (default is 2)')\u001b[39;00m\n\u001b[0;32m--> 953\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_net(save_path\u001b[39m=\u001b[39;49msave_path, save_every\u001b[39m=\u001b[39;49msave_every, save_each\u001b[39m=\u001b[39;49msave_each,\n\u001b[1;32m    954\u001b[0m                              learning_rate\u001b[39m=\u001b[39;49mlearning_rate, n_epochs\u001b[39m=\u001b[39;49mn_epochs, \n\u001b[1;32m    955\u001b[0m                              momentum\u001b[39m=\u001b[39;49mmomentum, weight_decay\u001b[39m=\u001b[39;49mweight_decay, patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m    956\u001b[0m                              SGD\u001b[39m=\u001b[39;49mSGD, batch_size\u001b[39m=\u001b[39;49mbatch_size, nimg_per_epoch\u001b[39m=\u001b[39;49mnimg_per_epoch, model_name\u001b[39m=\u001b[39;49mmodel_name)\n\u001b[1;32m    957\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretrained_model \u001b[39m=\u001b[39m model_path\n\u001b[1;32m    958\u001b[0m \u001b[39mreturn\u001b[39;00m model_path\n",
      "File \u001b[0;32m~/projects/cellpose/cellpose/models.py:798\u001b[0m, in \u001b[0;36mCellposeModel._train_net\u001b[0;34m(self, save_path, save_every, save_each, learning_rate, n_epochs, momentum, weight_decay, patience, SGD, batch_size, nimg_per_epoch, rescale, model_name)\u001b[0m\n\u001b[1;32m    796\u001b[0m tqdm_out \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mTqdmToLogger(models_logger, level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mINFO)\n\u001b[1;32m    797\u001b[0m \u001b[39mfor\u001b[39;00m imgi, lbl \u001b[39min\u001b[39;00m tqdm(tr_loader, file\u001b[39m=\u001b[39mtqdm_out, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00miepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step(imgi, lbl)\n\u001b[1;32m    799\u001b[0m     lavg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\n\u001b[1;32m    800\u001b[0m     nsum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(imgi) \n",
      "File \u001b[0;32m~/projects/cellpose/cellpose/core.py:749\u001b[0m, in \u001b[0;36mUnetModel._train_step\u001b[0;34m(self, x, lbl)\u001b[0m\n\u001b[1;32m    747\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(lbl,y)\n\u001b[1;32m    748\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 749\u001b[0m train_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    750\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    751\u001b[0m train_loss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start logger (to see training across epochs)\n",
    "logger = io.logger_setup()\n",
    "\n",
    "# DEFINE CELLPOSE MODEL (without size model)\n",
    "model = models.CellposeModel(train_loader, gpu=use_GPU, model_type=initial_model, diam_mean=27)\n",
    "\n",
    "# set channels\n",
    "channels = [chan, chan2]\n",
    "\n",
    "new_model_path = model.train(save_path=train_dir, \n",
    "                              n_epochs=n_epochs,\n",
    "                              learning_rate=learning_rate, \n",
    "                              weight_decay=weight_decay, \n",
    "                              nimg_per_epoch=5,\n",
    "                              model_name=model_name,\n",
    "                              )\n",
    "\n",
    "# diameter of labels in training images\n",
    "diam_labels = model.diam_labels.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdH0j8-L6FuB"
   },
   "source": [
    "## Evaluate on test data (optional)\n",
    "\n",
    "If you have test data, check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0AGsH5p6K6S",
    "outputId": "3f67063a-2197-4ec2-8619-0b52c98fd0ee"
   },
   "outputs": [],
   "source": [
    "# get files (during training, test_data is transformed so we will load it again)\n",
    "output = io.load_train_test_data(test_dir, mask_filter='_seg.npy')\n",
    "test_data, test_labels = output[:2]\n",
    "\n",
    "# run model on test images\n",
    "masks = model.eval(test_data, \n",
    "                   channels=[chan, chan2],\n",
    "                   diameter=diam_labels)[0]\n",
    "\n",
    "# check performance using ground truth labels\n",
    "ap = metrics.average_precision(test_labels, masks)[0]\n",
    "print('')\n",
    "print(f'>>> average precision at iou threshold 0.5 = {ap[:,0].mean():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8tZ8uYR-IFW"
   },
   "source": [
    "plot masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z2ac5gtr-HPq",
    "outputId": "65c96437-85e4-42cf-8d4b-414b6ba98c0a"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,8), dpi=150)\n",
    "for k,im in enumerate(test_data):\n",
    "    img = im.copy()\n",
    "    plt.subplot(3,len(train_files), k+1)\n",
    "    img = np.vstack((img, np.zeros_like(img)[:1]))\n",
    "    img = img.transpose(1,2,0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    if k==0:\n",
    "        plt.title('image')\n",
    "\n",
    "    plt.subplot(3,len(train_files), len(train_files) + k+1)\n",
    "    plt.imshow(masks[k])\n",
    "    plt.axis('off')\n",
    "    if k==0:\n",
    "        plt.title('predicted labels')\n",
    "\n",
    "    plt.subplot(3,len(train_files), 2*len(train_files) + k+1)\n",
    "    plt.imshow(test_labels[k])\n",
    "    plt.axis('off')\n",
    "    if k==0:\n",
    "        plt.title('true labels')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbVIZbNk5hgR"
   },
   "source": [
    "# Use custom model to segment images\n",
    "\n",
    "Take custom trained model from above, or upload your own model to google drive / colab runtime.\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vDu4Ixjo588O"
   },
   "outputs": [],
   "source": [
    "# model name and path\n",
    "\n",
    "#@markdown ###Custom model path (full path):\n",
    "\n",
    "model_path = \"human_in_the_loop/train/models/CP_tissuenet\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ###Path to images:\n",
    "\n",
    "dir = \"human_in_the_loop/test\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ###Channel Parameters:\n",
    "\n",
    "Channel_to_use_for_segmentation = \"Green\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "# @markdown If you have a secondary channel that can be used, for instance nuclei, choose it here:\n",
    "\n",
    "Second_segmentation_channel= \"Red\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "\n",
    "# Here we match the channel to number\n",
    "if Channel_to_use_for_segmentation == \"Grayscale\":\n",
    "  chan = 0\n",
    "elif Channel_to_use_for_segmentation == \"Blue\":\n",
    "  chan = 3\n",
    "elif Channel_to_use_for_segmentation == \"Green\":\n",
    "  chan = 2\n",
    "elif Channel_to_use_for_segmentation == \"Red\":\n",
    "  chan = 1\n",
    "\n",
    "\n",
    "if Second_segmentation_channel == \"Blue\":\n",
    "  chan2 = 3\n",
    "elif Second_segmentation_channel == \"Green\":\n",
    "  chan2 = 2\n",
    "elif Second_segmentation_channel == \"Red\":\n",
    "  chan2 = 1\n",
    "elif Second_segmentation_channel == \"None\":\n",
    "  chan2 = 0\n",
    "\n",
    "#@markdown ### Segmentation parameters:\n",
    "\n",
    "#@markdown diameter of cells (set to zero to use diameter from training set):\n",
    "diameter =  0#@param {type:\"number\"}\n",
    "#@markdown threshold on flow error to accept a mask (set higher to get more cells, e.g. in range from (0.1, 3.0), OR set to 0.0 to turn off so no cells discarded):\n",
    "flow_threshold = 0.4 #@param {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
    "#@markdown threshold on cellprob output to seed cell masks (set lower to include more pixels or higher to include fewer, e.g. in range from (-6, 6)):\n",
    "cellprob_threshold=0 #@param {type:\"slider\", min:-6, max:6, step:1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Axg2YQEpDx0e"
   },
   "source": [
    "if you're using the example test data we'll copy it to a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "InyKGtD3D2ZX",
    "outputId": "b1b4ecf3-41a0-4463-8944-afd8200cece0"
   },
   "outputs": [],
   "source": [
    "src = 'human_in_the_loop/test'\n",
    "if dir[:len(src)] == src:\n",
    "    files = io.get_image_files(dir, '_masks')\n",
    "    dir = 'human_in_the_loop/eval/'\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    for f in files:\n",
    "        dst = dir + os.path.split(f)[1]\n",
    "        print(f'{f} > {dst}')\n",
    "        shutil.copyfile(f, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JJ1q0nTBAAR"
   },
   "source": [
    "Here's what the command to train would be on the command line -- make sure if you run this locally to correct the paths for your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8P5voZOVM-H9",
    "outputId": "a9c9f1fb-7cf9-4676-bc1a-8a261f888274"
   },
   "outputs": [],
   "source": [
    "run_str = f'python -m cellpose --use_gpu --verbose --dir {dir} --pretrained_model {model_path} --chan {chan} --chan2 {chan2} --diameter {diameter} --flow_threshold {flow_threshold} --cellprob_threshold {cellprob_threshold}'\n",
    "print(run_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN3rdsfMBc_8"
   },
   "source": [
    "## run custom model\n",
    "\n",
    "how to run the custom model in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCcbs722BYd0",
    "outputId": "b7de466b-4e7a-4585-b1d7-c282593b3fab"
   },
   "outputs": [],
   "source": [
    "# gets image files in dir (ignoring image files ending in _masks)\n",
    "files = io.get_image_files(dir, '_masks')\n",
    "print(files)\n",
    "images = [io.imread(f) for f in files]\n",
    "\n",
    "# declare model\n",
    "model = models.CellposeModel(gpu=True, \n",
    "                             pretrained_model=model_path)\n",
    "\n",
    "# use model diameter if user diameter is 0\n",
    "diameter = model.diam_labels if diameter==0 else diameter\n",
    "\n",
    "# run model on test images\n",
    "masks, flows, styles = model.eval(images, \n",
    "                                  channels=[chan, chan2],\n",
    "                                  diameter=diameter,\n",
    "                                  flow_threshold=flow_threshold,\n",
    "                                  cellprob_threshold=cellprob_threshold\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj5AIZ825o7P"
   },
   "source": [
    "## save output to *_seg.npy\n",
    "\n",
    "you will see the files save in the Files tab and you can download them from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc7EWe_f5oEH"
   },
   "outputs": [],
   "source": [
    "from cellpose import io\n",
    "\n",
    "io.masks_flows_to_seg(images, \n",
    "                      masks, \n",
    "                      flows, \n",
    "                      diameter*np.ones(len(masks)), \n",
    "                      files, \n",
    "                      [chan, chan2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwoUuuarC9V5"
   },
   "source": [
    "## save output masks to tiffs/pngs or txt files for imageJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da-Rtx09DEZB"
   },
   "outputs": [],
   "source": [
    "io.save_masks(images, \n",
    "              masks, \n",
    "              flows, \n",
    "              files, \n",
    "              channels=[chan, chan2],\n",
    "              png=True, # save masks as PNGs and save example image\n",
    "              tif=True, # save masks as TIFFs\n",
    "              save_txt=True, # save txt outlines for ImageJ\n",
    "              save_flows=False, # save flows as TIFFs\n",
    "              save_outlines=False, # save outlines as TIFFs \n",
    "              )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "PiP9MWN4F3Sx",
    "outputId": "e70088bf-dbb2-4b49-a5df-620c2b253d68"
   },
   "outputs": [],
   "source": [
    "f = files[0]\n",
    "plt.figure(figsize=(12,4), dpi=300)\n",
    "plt.imshow(io.imread(os.path.splitext(f)[0] + '_cp_output.png'))\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
